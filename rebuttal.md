### Rebuttal towards Weaknesses:

> On the claim of 'dimension-aware'.

We have now added new experiments on the TorusVisForce dataset to showcase why the model is dimension-aware:

In Section 4.5, we introduced a method to generate 'similar' datasets from training data through dimensional analysis. We will refer to this generating process as a 'similar transformation'.

If we consider the ground-truth operator that the Neural Operator method needs to approximate, the invariance of similar transformation is a crucial property according to dimensional analysis. However, if we view this dataset from a traditional machine learning perspective (without accounting for operator learning), the generated data can be regarded as out-of-distribution. 

We present the specific experimental configurations and corresponding results below. As shown, the DimOL models consistently achieve **over 11% improvements** compared to LSM on OOD data generated through 'similar transformations', which exceeds the performance gain observed on the in-distribution test set (3.3%).


This property is particularly beneficial when combining machine learning with traditional dimensional analysis. Specifically, it allows us to collect data on smaller-scale simulations and then transfer the model onto real-world normal-scale data, which is more challenging to acquire than small-scale simulations.

This will be introduced as a new section of the paper. Again, thanks for pointing out the weakness of our work!

#### Experimental Settings:

TorusVisForceTopMu: This is a subset of the TorusVisForce dataset, containing 200 pieces of data with the highest viscosity values (shuffled). The viscosity range for this subset is [8.2e-05, 1e-4].


Train/Test Splits:
- Training: The first 100 samples of TorusVisForceTopMu.
- In-set testing: The in-distribution test set, the last 100 samples of TorusVisForceTopMu.
- OOD testing: Out-of-distribution test set generated by transforming the training data using a scaling coefficient $k$.

| TorusVisForceTopMu (T=10)|MSE (In-set)|MSE (OOD, k=4)|MSE (OOD, k=16)|
|-|-|-|-|
|LSM|0.08304 |0.01427|0.01500|
|LSM + ProdLayer (p=2)|0.08031|0.01262|0.01334   |
|Promotion|3.3%|11.6%|11.1%|


### Q&A
> 1. Why on TorisVisForce with T-FNO+ProdLayer have biggest gain?

After careful examination, we found that the 74.3% performance gain is primarily attributable to the use of a baseline T-FNO model without the channel-mixing MLP (which is an optional component in the official T-FNO codebase). In this rebuttal, we conduct further experiments by incorporating the channel-mixing MLP (which has more parameters than ProdLayer) into the baseline model. Please refer to our **General Response (Correction 1)** for the updated results.

> 2. Consistency/significance of the performance boosts with the ProdLayer Robustness

First, we would like to summarize the performance gain achieved by using ProdLayer in comparison to FNO-based baseline models that incorporate channel-mixing MLPs. The average improvement across the 6 benchmarks is approximately **15%**. 

It is noteworthy that, since we applied the MLP (which has more parameters than ProdLayer) in the baseline model, having a 15% performance gain without increasing the number of parameters is a remarkable achievement.

|Burgers|DarcyFlow|TorusLi|TorusVisForce (T=4)|TorusVisForce (T=10)|Few-shot TorusVisForce|Average|
|-|-|-|-|-|-|-|
|48.4%|6.9% |1.5%|11.5%|5.5%|16.5%|15.1%|

Note: The TorusLi data has only one input channel, while the performance gain of DimOL is largely attributed to its ability to handle relationships between different input variables. This explains why DimOL achieves only a 1.5% improvement on this dataset.

Further, as suggested by the reviewer, we train the models 4 times using random initialization seeds, and observe a standard deviation much smaller than the improvement ratio: in Burgers and TorusVisForce, respectively 43.3% and 5.5%, the results are nshown below. The performance gain of DimOL against different compared models remains consistent. Notably, the DimOL models are actually more stable than baselines.


|Burgers MSE(1e-3)|1|2|3|4|mean|std/mean|
|-|-|-|-|-|-|-|
|FNO (p=0)|2.264|2.346|2.476|2.280|2.342|3.6%|
|FNO (p=2)|1.126|1.184|1.141|1.139|1.147|1.9%|


|TorusVisForce MSE(1e-2)|1|2|3|4|mean|std/mean|
|-|-|-|-|-|-|-|
|T-FNO (p=0)|1.744|1.737|1.673|1.715|1.717|1.6%|
|T-FNO (p=2)|1.634|1.638|1.621|1.627|1.630|0.4%|

Thanks for your advice to improve the experiment! 



> 3. Models with ProdLayer have slightly more parameters than the ones without (Tables 1, 2). Is it possible that the slight increase in size explains the boost in performance?

First, as mentioned earlier, the ProdLayer introduces an average performance gain of 15% compared to existing FNO-based models that incorporate channel-mixing MLPs with a similar number of parameters.

Additionally, per the reviewer's request, we have tested the performance of FNO and T-FNO baseline models with different numbers of MLP layers, within the range $L_{MLP} \in$ \{2, 3, 4, 5, 6\}. The results indicate that a 2 or 3 layer MLP would the best choice for the baseline. Baselines with 4 or more layers have a larger number of parameters than those with 2 layers, while simply increasing the parameter count in the MLP of FNO-based baselines does not necessarily lead to improved performance. In TorusVisForce case, though baseline with $L_{MLP}=3$ outperforms $L_{MLP}=2$, the parameter amount is slightly higher than T-FNO+ProdLayer, and the improvement is incremental.

Below, we present the results on the DarcyFlow and TorusVisForce dataset for T-FNO.

|$L_{MLP}$ |2|3|4|5|6|ProdLayer (p=2)|
|-|-|-|-|-|-|-|
|Param (Millon)|0.6882|0.6893|0.6904|0.6915|0.6925|0.6883|
|DarcyFlow MSE (1e-3)|7.500|7.951|8.630|9.373|8.165|6.919|
|Few-shot TorusVisForce MSE (1e-2)|2.104|2.097|2.142|2.145|2.180|1.757|
