{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Only supports the time series datasets! (model definition assumes it's 2d problem here)\n",
    "    (the train numbers and test numbers should not be predefined in the dataset, otherwise merge the dataset)\n",
    "    the predicted feature now only support 1 channel\n",
    "    logger currently only supports tensorboard \n",
    "    TODO: support more channels\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from neuralop.models import FNO\n",
    "# from neuralop.training import OutputEncoderCallback\n",
    "from neuralop.utils import count_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.datasets.autoregressive_dataset import load_autoregressive_traintestsplit_v3\n",
    "from neuralop.training import MultipleInputCallback, SimpleTensorBoardLoggerCallback, ModelCheckpointCallback\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser('FNO Models', add_help=False)\n",
    "    parser.add_argument('--model', type=str, default='FNO')\n",
    "    parser.add_argument('--model_name',  type=str, default='FNO')\n",
    "    # # # Data Loader Configs # # #\n",
    "    parser.add_argument('--n_train', type=int, default=2)\n",
    "    parser.add_argument('--n_test', nargs='+', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=32) #\n",
    "    parser.add_argument('--test_batch_size', type=int, default=128)\n",
    "    parser.add_argument('--train_subsample_rate', type=int, default=4)\n",
    "    parser.add_argument('--test_subsample_rate', nargs='+',  type=int, default=4)\n",
    "    parser.add_argument('--time_step', type=int, default=10)\n",
    "    parser.add_argument('--predict_feature', type=str, default='u')\n",
    "    parser.add_argument('--data_path', type=str, default='./data/ns_random_forces_1.h5', help=\"the path of data file\")\n",
    "    parser.add_argument('--test_data_path', nargs='+', type=str, default='', help=\"the path of test data file\")\n",
    "    parser.add_argument('--data_name', type=str, default='NS_Contextual', help=\"the name of dataset\")\n",
    "    parser.add_argument('--simaug_train_data', type=bool, default=False, help=\"whether to augment the dataset with similar ones\")\n",
    "    parser.add_argument('--simaug_test_data', type=bool, default=False, help=\"whether to augment the test dataset with similar ones\")\n",
    "    # # # Model Configs # # #\n",
    "    parser.add_argument('--n_modes', type=int, default=21) #\n",
    "    parser.add_argument('--num_prod', type=int, default=2) #\n",
    "    parser.add_argument('--n_layers', type=int, default=4) ##\n",
    "    parser.add_argument('--raw_in_channels', type=int, default=3, help='TorusLi: 1; ns_contextual: 3')\n",
    "    parser.add_argument('--pos_encoding', type=bool, default=True) ##\n",
    "    parser.add_argument('--hidden_channels', type=int, default=32) #\n",
    "    parser.add_argument('--lifting_channels', type=int, default=256) #\n",
    "    parser.add_argument('--projection_channels', type=int, default=64) #\n",
    "    parser.add_argument('--factorization', type=str, default='tucker') #####\n",
    "    parser.add_argument('--channel_mixing', type=str, default='', help='') #####\n",
    "    parser.add_argument('--rank', type=float, default=0.42, help='the compression rate of tensor') #\n",
    "    parser.add_argument('--load_path', type=str, default='', help='load checkpoint')\n",
    "\n",
    "    # # # Optimizer Configs # # #\n",
    "    parser.add_argument('--lr', type=float, default=1e-3) #Path\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4) #\n",
    "    parser.add_argument('--scheduler_steps', type=int, default=100) #\n",
    "    parser.add_argument('--scheduler_gamma', type=float, default=0.5) #\n",
    "    parser.add_argument('--train_loss', type=str, default='h1', help='h1 or l2') #\n",
    "    # # # Log and Save Configs # # #\n",
    "    parser.add_argument('--log_path', type=str, default='./runs')\n",
    "    parser.add_argument('--save_path', type=str, default='./ckpt')\n",
    "    parser.add_argument('--prefix', type=str, default='', help='prefix of log and save file')\n",
    "    parser.add_argument('--time_suffix', type=bool, default=True, help='whether to use program start time as suffix')\n",
    "    parser.add_argument('--config_details', type=bool, default=True, help='whether to include config details to the log and save file name')\n",
    "    parser.add_argument('--log_interval', type=int, default=4)\n",
    "    parser.add_argument('--save_interval', type=int, default=20)\n",
    "    # # # Trainer Configs # # #\n",
    "    parser.add_argument('--epochs', type=int, default=501) #\n",
    "    parser.add_argument('--verbose', type=bool, default=True)\n",
    "    parser.add_argument('--random_seed', type=bool, default=False)\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def run(args):\n",
    "    seed = args.seed\n",
    "    if args.random_seed:\n",
    "        import random\n",
    "        seed = random.randint(1, 10000)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    verbose = args.verbose\n",
    "    # # # Data Preparation # # #\n",
    "    n_train = args.n_train\n",
    "    n_test = args.n_test\n",
    "    batch_size = args.batch_size\n",
    "    test_batch_size = args.test_batch_size\n",
    "    train_subsample_rate = args.train_subsample_rate\n",
    "    test_subsample_rate = args.test_subsample_rate\n",
    "    time_step = args.time_step\n",
    "    data_path = args.data_path\n",
    "    train_loader, test_loaders = load_autoregressive_traintestsplit_v3(\n",
    "        data_path,\n",
    "        n_train, n_test,\n",
    "        batch_size, test_batch_size,\n",
    "        train_subsample_rate, test_subsample_rate,\n",
    "        time_step,\n",
    "        test_data_paths=args.test_data_path,\n",
    "        predict_feature=args.predict_feature,\n",
    "        append_positional_encoding=args.pos_encoding\n",
    "    )\n",
    "    resolution = train_loader.dataset[0]['x'].shape[0]\n",
    "\n",
    "    # # # Model Definition # # #\n",
    "    n_modes=args.n_modes\n",
    "    num_prod=args.num_prod\n",
    "    in_channels = args.raw_in_channels\n",
    "    if args.pos_encoding:\n",
    "        in_channels += 2\n",
    "    model = FNO(in_channels=in_channels, n_modes=(n_modes, n_modes), hidden_channels=args.hidden_channels, lifting_channels=args.lifting_channels,\n",
    "                projection_channels=args.projection_channels, n_layers=args.n_layers, factorization=args.factorization, channel_mixing=args.channel_mixing, rank=args.rank, num_prod=num_prod)\n",
    "    \n",
    "    if args.load_path != '':\n",
    "        model.load_state_dict(torch.load(args.load_path))\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    n_params = count_params(model)\n",
    "    print(f'\\nOur model has {n_params} parameters.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # # # Optimizer Definition # # #\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                    lr=args.lr, \n",
    "                                    weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.scheduler_steps, gamma=args.scheduler_gamma)\n",
    "\n",
    "    # # # Loss Definition # # #\n",
    "    l2loss = LpLoss(d=2, p=2)\n",
    "    h1loss = H1Loss(d=2)\n",
    "\n",
    "    if args.train_loss == 'h1':\n",
    "        train_loss = h1loss\n",
    "    elif args.train_loss == 'l2':\n",
    "        train_loss = l2loss\n",
    "    else: assert False, \"Unsupported training loss!\"\n",
    "    eval_losses={'h1': h1loss, 'l2': l2loss}\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n### MODEL ###\\n', model)\n",
    "        print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "        print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "        print('\\n### LOSSES ###')\n",
    "        print(f'\\n * Train: {train_loss}')\n",
    "        print(f'\\n * Test: {eval_losses}')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # # # Logs and Saves Definition (path and file name) # # #\n",
    "    if not os.path.exists(args.log_path):\n",
    "        os.makedirs(args.log_path)\n",
    "    if not os.path.exists(args.save_path):\n",
    "        os.makedirs(args.save_path)\n",
    "    file_name = f'{args.data_name}_{args.model_name}'\n",
    "    prefix = args.prefix\n",
    "    if prefix != '': file_name = file_name + '_' + prefix\n",
    "    # config_name = ''\n",
    "    config_file_path=''\n",
    "    if args.config_details:\n",
    "        # config_name = f'_b{args.batch_size}_mode{args.n_modes}_prod{args.num_prod}_layer{args.n_layers}_hid{args.hidden_channels}_lift{args.lifting_channels}_proj{args.projection_channels}_fact-{args.factorization}_rank{args.rank}_mix-{args.channel_mixing}_pos-enc-{args.pos_encoding}_lr{args.lr}_wd{args.weight_decay}_sche-step{args.scheduler_steps}_gamma{args.scheduler_gamma}_loss{args.train_loss}'\n",
    "        config_file_path = f\"/timestep_{args.time_step}/layer_{args.n_layers}/fact-{args.factorization}/rank_{args.rank}/mix-{args.channel_mixing}/prod_{args.num_prod}/pos-enc-{args.pos_encoding}/loss-{args.train_loss}/mode_{args.n_modes}/hid_{args.hidden_channels}/lift_{args.lifting_channels}/proj_{args.projection_channels}/b_{args.batch_size}/lr_{args.lr}/wd_{args.weight_decay}/sche-step_{args.scheduler_steps}/gamma_{args.scheduler_gamma}/simaug_train_data_{args.simaug_train_data}/\"\n",
    "    time_name = ''\n",
    "    if args.time_suffix:\n",
    "        localtime = time.localtime(time.time())\n",
    "        time_name = f\"{localtime.tm_mon}-{localtime.tm_mday}-{localtime.tm_hour}-{localtime.tm_min}\"\n",
    "    # file_name = file_name + config_name + time_name\n",
    "    file_name = file_name + config_file_path + time_name\n",
    "\n",
    "    log_dir = args.log_path\n",
    "    if log_dir[-1]!='/': log_dir = log_dir + '/'\n",
    "    log_dir = log_dir + file_name\n",
    "    save_dir = args.save_path\n",
    "    if save_dir[-1]!='/': save_dir = save_dir + '/'\n",
    "    save_dir = save_dir + file_name\n",
    "\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    log_dir = Path(log_dir)\n",
    "    save_dir = Path(save_dir)\n",
    "\n",
    "    # # # Trainer Definition # # #\n",
    "    from scripts.ns_contextual_trainer import ns_contextual_trainer\n",
    "\n",
    "    trainer = ns_contextual_trainer(model=model, n_epochs=args.epochs,\n",
    "                    device=device,\n",
    "                    simaug_test_data=args.simaug_test_data,\n",
    "                    simaug_train_data=args.simaug_train_data,\n",
    "                    callbacks=[SimpleTensorBoardLoggerCallback(log_dir=log_dir),\n",
    "                               ModelCheckpointCallback(\n",
    "                                checkpoint_dir=save_dir,\n",
    "                                interval=args.save_interval)],\n",
    "                    scaling_ks=[1,], scaling_ps=[4,8,16],\n",
    "                    wandb_log=False,\n",
    "                    log_test_interval=args.log_interval,\n",
    "                    use_distributed=False,\n",
    "                    verbose=True)\n",
    "\n",
    "    trainer.train(train_loader=train_loader,\n",
    "                test_loaders=test_loaders,\n",
    "                optimizer=optimizer, \n",
    "                scheduler=scheduler, \n",
    "                regularizer=False, \n",
    "                training_loss=train_loss, \n",
    "                eval_losses=eval_losses)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/yichen/repo/cfd/myNeuralOperator/ckpt/TorusLi_LSM/width_32/pos-enc-True/num_token4/num_basis_12/patch_size_4,4/padding_0,0/loss-h1/b_20/lr_0.001/wd_0.0001/sche-step_100/gamma_0.5/5-17-22-3/ep_480.pt'\n",
    "import torch\n",
    "path = '/home/yichen/repo/cfd/myNeuralOperator/ckpt/NS_Contextual_Fewshot_LSM/width_32/pos-enc-True/num_token4/num_basis_12/patch_size_4,4/padding_0,0/mix-/loss-h1/b_64/lr_0.001/wd_0.0001/sche-step_100/gamma_0.5/5-20-17-26/ep_480.pt'\n",
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'process4.encoder_attn.linear.fcs.0.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# state_dict['process4.encoder_attn.weight'].shape\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocess4.encoder_attn.linear.fcs.0.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 'process4.encoder_attn.linear.fcs.0.weight'"
     ]
    }
   ],
   "source": [
    "# state_dict['process4.encoder_attn.weight'].shape\n",
    "state_dict['process4.encoder_attn.linear.fcs.0.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['inc.double_conv.0.weight', 'inc.double_conv.1.weight', 'inc.double_conv.1.bias', 'inc.double_conv.1.running_mean', 'inc.double_conv.1.running_var', 'inc.double_conv.1.num_batches_tracked', 'inc.double_conv.3.weight', 'inc.double_conv.4.weight', 'inc.double_conv.4.bias', 'inc.double_conv.4.running_mean', 'inc.double_conv.4.running_var', 'inc.double_conv.4.num_batches_tracked', 'down1.maxpool_conv.1.double_conv.0.weight', 'down1.maxpool_conv.1.double_conv.1.weight', 'down1.maxpool_conv.1.double_conv.1.bias', 'down1.maxpool_conv.1.double_conv.1.running_mean', 'down1.maxpool_conv.1.double_conv.1.running_var', 'down1.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down1.maxpool_conv.1.double_conv.3.weight', 'down1.maxpool_conv.1.double_conv.4.weight', 'down1.maxpool_conv.1.double_conv.4.bias', 'down1.maxpool_conv.1.double_conv.4.running_mean', 'down1.maxpool_conv.1.double_conv.4.running_var', 'down1.maxpool_conv.1.double_conv.4.num_batches_tracked', 'down2.maxpool_conv.1.double_conv.0.weight', 'down2.maxpool_conv.1.double_conv.1.weight', 'down2.maxpool_conv.1.double_conv.1.bias', 'down2.maxpool_conv.1.double_conv.1.running_mean', 'down2.maxpool_conv.1.double_conv.1.running_var', 'down2.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down2.maxpool_conv.1.double_conv.3.weight', 'down2.maxpool_conv.1.double_conv.4.weight', 'down2.maxpool_conv.1.double_conv.4.bias', 'down2.maxpool_conv.1.double_conv.4.running_mean', 'down2.maxpool_conv.1.double_conv.4.running_var', 'down2.maxpool_conv.1.double_conv.4.num_batches_tracked', 'down3.maxpool_conv.1.double_conv.0.weight', 'down3.maxpool_conv.1.double_conv.1.weight', 'down3.maxpool_conv.1.double_conv.1.bias', 'down3.maxpool_conv.1.double_conv.1.running_mean', 'down3.maxpool_conv.1.double_conv.1.running_var', 'down3.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down3.maxpool_conv.1.double_conv.3.weight', 'down3.maxpool_conv.1.double_conv.4.weight', 'down3.maxpool_conv.1.double_conv.4.bias', 'down3.maxpool_conv.1.double_conv.4.running_mean', 'down3.maxpool_conv.1.double_conv.4.running_var', 'down3.maxpool_conv.1.double_conv.4.num_batches_tracked', 'down4.maxpool_conv.1.double_conv.0.weight', 'down4.maxpool_conv.1.double_conv.1.weight', 'down4.maxpool_conv.1.double_conv.1.bias', 'down4.maxpool_conv.1.double_conv.1.running_mean', 'down4.maxpool_conv.1.double_conv.1.running_var', 'down4.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down4.maxpool_conv.1.double_conv.3.weight', 'down4.maxpool_conv.1.double_conv.4.weight', 'down4.maxpool_conv.1.double_conv.4.bias', 'down4.maxpool_conv.1.double_conv.4.running_mean', 'down4.maxpool_conv.1.double_conv.4.running_var', 'down4.maxpool_conv.1.double_conv.4.num_batches_tracked', 'up1.conv.double_conv.0.weight', 'up1.conv.double_conv.1.weight', 'up1.conv.double_conv.1.bias', 'up1.conv.double_conv.1.running_mean', 'up1.conv.double_conv.1.running_var', 'up1.conv.double_conv.1.num_batches_tracked', 'up1.conv.double_conv.3.weight', 'up1.conv.double_conv.4.weight', 'up1.conv.double_conv.4.bias', 'up1.conv.double_conv.4.running_mean', 'up1.conv.double_conv.4.running_var', 'up1.conv.double_conv.4.num_batches_tracked', 'up2.conv.double_conv.0.weight', 'up2.conv.double_conv.1.weight', 'up2.conv.double_conv.1.bias', 'up2.conv.double_conv.1.running_mean', 'up2.conv.double_conv.1.running_var', 'up2.conv.double_conv.1.num_batches_tracked', 'up2.conv.double_conv.3.weight', 'up2.conv.double_conv.4.weight', 'up2.conv.double_conv.4.bias', 'up2.conv.double_conv.4.running_mean', 'up2.conv.double_conv.4.running_var', 'up2.conv.double_conv.4.num_batches_tracked', 'up3.conv.double_conv.0.weight', 'up3.conv.double_conv.1.weight', 'up3.conv.double_conv.1.bias', 'up3.conv.double_conv.1.running_mean', 'up3.conv.double_conv.1.running_var', 'up3.conv.double_conv.1.num_batches_tracked', 'up3.conv.double_conv.3.weight', 'up3.conv.double_conv.4.weight', 'up3.conv.double_conv.4.bias', 'up3.conv.double_conv.4.running_mean', 'up3.conv.double_conv.4.running_var', 'up3.conv.double_conv.4.num_batches_tracked', 'up4.conv.double_conv.0.weight', 'up4.conv.double_conv.1.weight', 'up4.conv.double_conv.1.bias', 'up4.conv.double_conv.1.running_mean', 'up4.conv.double_conv.1.running_var', 'up4.conv.double_conv.1.num_batches_tracked', 'up4.conv.double_conv.3.weight', 'up4.conv.double_conv.4.weight', 'up4.conv.double_conv.4.bias', 'up4.conv.double_conv.4.running_mean', 'up4.conv.double_conv.4.running_var', 'up4.conv.double_conv.4.num_batches_tracked', 'outc.conv.weight', 'outc.conv.bias', 'process1.weights', 'process1.latent', 'process1.encoder_attn.weight', 'process1.encoder_attn.bias', 'process1.decoder_attn.weight', 'process1.decoder_attn.bias', 'process2.weights', 'process2.latent', 'process2.encoder_attn.weight', 'process2.encoder_attn.bias', 'process2.decoder_attn.weight', 'process2.decoder_attn.bias', 'process3.weights', 'process3.latent', 'process3.encoder_attn.weight', 'process3.encoder_attn.bias', 'process3.decoder_attn.weight', 'process3.decoder_attn.bias', 'process4.weights', 'process4.latent', 'process4.encoder_attn.weight', 'process4.encoder_attn.bias', 'process4.decoder_attn.weight', 'process4.decoder_attn.bias', 'process5.weights', 'process5.latent', 'process5.encoder_attn.weight', 'process5.encoder_attn.bias', 'process5.decoder_attn.weight', 'process5.decoder_attn.bias', 'fc0.weight', 'fc0.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "odict_keys(['inc.double_conv.0.weight', 'inc.double_conv.1.weight', 'inc.double_conv.1.bias', 'inc.double_conv.1.running_mean', 'inc.double_conv.1.running_var', 'inc.double_conv.1.num_batches_tracked', 'inc.double_conv.3.weight', 'inc.double_conv.4.weight', 'inc.double_conv.4.bias', 'inc.double_conv.4.running_mean', 'inc.double_conv.4.running_var', 'inc.double_conv.4.num_batches_tracked', 'down1.maxpool_conv.1.double_conv.0.weight', 'down1.maxpool_conv.1.double_conv.1.weight', 'down1.maxpool_conv.1.double_conv.1.bias', 'down1.maxpool_conv.1.double_conv.1.running_mean', 'down1.maxpool_conv.1.double_conv.1.running_var', 'down1.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down1.maxpool_conv.1.double_conv.3.weight', 'down1.maxpool_conv.1.double_conv.4.weight', 'down1.maxpool_conv.1.double_conv.4.bias', 'down1.maxpool_conv.1.double_conv.4.running_mean', 'down1.maxpool_conv.1.double_conv.4.running_var', 'down1.maxpool_conv.1.double_conv.4.num_batches_tracked', 'down2.maxpool_conv.1.double_conv.0.weight', 'down2.maxpool_conv.1.double_conv.1.weight', 'down2.maxpool_conv.1.double_conv.1.bias', 'down2.maxpool_conv.1.double_conv.1.running_mean', 'down2.maxpool_conv.1.double_conv.1.running_var', 'down2.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down2.maxpool_conv.1.double_conv.3.weight', 'down2.maxpool_conv.1.double_conv.4.weight', 'down2.maxpool_conv.1.double_conv.4.bias', 'down2.maxpool_conv.1.double_conv.4.running_mean', 'down2.maxpool_conv.1.double_conv.4.running_var', 'down2.maxpool_conv.1.double_conv.4.num_batches_tracked', 'down3.maxpool_conv.1.double_conv.0.weight', 'down3.maxpool_conv.1.double_conv.1.weight', 'down3.maxpool_conv.1.double_conv.1.bias', 'down3.maxpool_conv.1.double_conv.1.running_mean', 'down3.maxpool_conv.1.double_conv.1.running_var', 'down3.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down3.maxpool_conv.1.double_conv.3.weight', 'down3.maxpool_conv.1.double_conv.4.weight', 'down3.maxpool_conv.1.double_conv.4.bias', 'down3.maxpool_conv.1.double_conv.4.running_mean', 'down3.maxpool_conv.1.double_conv.4.running_var', 'down3.maxpool_conv.1.double_conv.4.num_batches_tracked', 'down4.maxpool_conv.1.double_conv.0.weight', 'down4.maxpool_conv.1.double_conv.1.weight', 'down4.maxpool_conv.1.double_conv.1.bias', 'down4.maxpool_conv.1.double_conv.1.running_mean', 'down4.maxpool_conv.1.double_conv.1.running_var', 'down4.maxpool_conv.1.double_conv.1.num_batches_tracked', 'down4.maxpool_conv.1.double_conv.3.weight', 'down4.maxpool_conv.1.double_conv.4.weight', 'down4.maxpool_conv.1.double_conv.4.bias', 'down4.maxpool_conv.1.double_conv.4.running_mean', 'down4.maxpool_conv.1.double_conv.4.running_var', 'down4.maxpool_conv.1.double_conv.4.num_batches_tracked', 'up1.conv.double_conv.0.weight', 'up1.conv.double_conv.1.weight', 'up1.conv.double_conv.1.bias', 'up1.conv.double_conv.1.running_mean', 'up1.conv.double_conv.1.running_var', 'up1.conv.double_conv.1.num_batches_tracked', 'up1.conv.double_conv.3.weight', 'up1.conv.double_conv.4.weight', 'up1.conv.double_conv.4.bias', 'up1.conv.double_conv.4.running_mean', 'up1.conv.double_conv.4.running_var', 'up1.conv.double_conv.4.num_batches_tracked', 'up2.conv.double_conv.0.weight', 'up2.conv.double_conv.1.weight', 'up2.conv.double_conv.1.bias', 'up2.conv.double_conv.1.running_mean', 'up2.conv.double_conv.1.running_var', 'up2.conv.double_conv.1.num_batches_tracked', 'up2.conv.double_conv.3.weight', 'up2.conv.double_conv.4.weight', 'up2.conv.double_conv.4.bias', 'up2.conv.double_conv.4.running_mean', 'up2.conv.double_conv.4.running_var', 'up2.conv.double_conv.4.num_batches_tracked', 'up3.conv.double_conv.0.weight', 'up3.conv.double_conv.1.weight', 'up3.conv.double_conv.1.bias', 'up3.conv.double_conv.1.running_mean', 'up3.conv.double_conv.1.running_var', 'up3.conv.double_conv.1.num_batches_tracked', 'up3.conv.double_conv.3.weight', 'up3.conv.double_conv.4.weight', 'up3.conv.double_conv.4.bias', 'up3.conv.double_conv.4.running_mean', 'up3.conv.double_conv.4.running_var', 'up3.conv.double_conv.4.num_batches_tracked', 'up4.conv.double_conv.0.weight', 'up4.conv.double_conv.1.weight', 'up4.conv.double_conv.1.bias', 'up4.conv.double_conv.1.running_mean', 'up4.conv.double_conv.1.running_var', 'up4.conv.double_conv.1.num_batches_tracked', 'up4.conv.double_conv.3.weight', 'up4.conv.double_conv.4.weight', 'up4.conv.double_conv.4.bias', 'up4.conv.double_conv.4.running_mean', 'up4.conv.double_conv.4.running_var', 'up4.conv.double_conv.4.num_batches_tracked', 'outc.conv.weight', 'outc.conv.bias', 'process1.weights', 'process1.latent', 'process1.encoder_attn.weight', 'process1.encoder_attn.bias', 'process1.decoder_attn.weight', 'process1.decoder_attn.bias', 'process2.weights', 'process2.latent', 'process2.encoder_attn.weight', 'process2.encoder_attn.bias', 'process2.decoder_attn.weight', 'process2.decoder_attn.bias', 'process3.weights', 'process3.latent', 'process3.encoder_attn.weight', 'process3.encoder_attn.bias', 'process3.decoder_attn.weight', 'process3.decoder_attn.bias', 'process4.weights', 'process4.latent', 'process4.encoder_attn.weight', 'process4.encoder_attn.bias', 'process4.decoder_attn.weight', 'process4.decoder_attn.bias', 'process5.weights', 'process5.latent', 'process5.encoder_attn.weight', 'process5.encoder_attn.bias', 'process5.decoder_attn.weight', 'process5.decoder_attn.bias', 'fc0.weight', 'fc0.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
