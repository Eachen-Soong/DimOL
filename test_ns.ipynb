{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl, I'm fucking cumming! you're the 515-th girl I've fucked.\n",
      "          You'll recieve 114 mL of my cum.\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "class fucker:\n",
    "    def __init__(self, cum_amount, fuck_cunt_cnt):\n",
    "        self.cum_amount = cum_amount\n",
    "        self.fuck_cunt_cnt = fuck_cunt_cnt\n",
    "\n",
    "    def fuck(self, girl:str):\n",
    "        print(f\"Yes!!!{self.cum_amount}\")\n",
    "        self.fuck_cunt_cnt +=1\n",
    "\n",
    "Qiyu = fucker(114, 514)\n",
    "\n",
    "def new_fuck(self, girl:str):\n",
    "    self.fuck_cunt_cnt +=1\n",
    "    print(f\"{girl}, I'm fucking cumming! you're the {self.fuck_cunt_cnt}-th girl I've fucked.\\n\\\n",
    "          You'll recieve {self.cum_amount} mL of my cum.\")\n",
    "\n",
    "Qiyu.fuck = types.MethodType(new_fuck, Qiyu)\n",
    "\n",
    "Qiyu.fuck(\"Girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm fucking horny!\n",
      "Yes!!!114\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class new_fucker(fucker):\n",
    "    def __init__(self, cum_amount, fuck_cunt_cnt):\n",
    "        super().__init__(cum_amount, fuck_cunt_cnt)\n",
    "        self.old_fuck = copy.copy(self.fuck)\n",
    "        def fuck(girl:str):\n",
    "            print(\"I'm fucking horny!\")\n",
    "            self.old_fuck(girl)\n",
    "        self.fuck = fuck\n",
    "\n",
    "me = new_fucker(114, 514)\n",
    "\n",
    "me.fuck(\"Girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(me, 'old_fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import FNO\n",
    "\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import OutputEncoderCallback\n",
    "from neuralop.utils import count_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "from neuralop.datasets.autoregressive_dataset import load_autoregressive_traintestsplit\n",
    "data_path = \"/home/yichen/repo/cfd/myFNO/data/zongyi/NavierStokes_V1e-5_N1200_T20.mat\"\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 200\n",
    "batch_size = 32\n",
    "test_batch_size = 64\n",
    "train_subsample_rate = 1\n",
    "test_subsample_rate = 1\n",
    "time_step = 1\n",
    "train_loader, test_loader = load_autoregressive_traintestsplit(\n",
    "    data_path,\n",
    "    n_train, n_test,\n",
    "    batch_size, test_batch_size, \n",
    "    train_subsample_rate, test_subsample_rate,\n",
    "    time_step,\n",
    "    predict_feature='u',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we would first establish a similar dataset adapter.\n",
    "\n",
    "input: train loader\n",
    "\n",
    "output: a new test loader\n",
    "\n",
    "transform rule:\n",
    "u1 = u / k; \n",
    "visc1 = visc / k; \n",
    "f1 = f / k**2; \n",
    "\n",
    "index rule: \n",
    "if the k list has length $n_k$, then for idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TFNO_prodlayer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our TFNO_prodlayer model has 1658657 parameters.\n"
     ]
    }
   ],
   "source": [
    "# We create a tensorized FNO model\n",
    "\n",
    "input_prods = None\n",
    "n_modes=21\n",
    "\n",
    "model = FNO(in_channels=3, n_modes=(n_modes, n_modes), hidden_channels=32, \n",
    "             projection_channels=64, factorization=None, channel_mixing='prod-layer', stabilizer='tanh', rank=0.)\n",
    "\n",
    "# method_name = type(model).__name__.split('.')[-1]\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_params(model)\n",
    "print(f'\\nOur '+model_name+f' model has {n_params} parameters.')\n",
    "\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MODEL ###\n",
      " FNO(\n",
      "  (fno_blocks): FNOBlocks1(\n",
      "    (convs): SpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-7): 8 x ComplexDenseTensor(shape=torch.Size([32, 32, 10, 10]), rank=None)\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (channel_mixer): ModuleList(\n",
      "      (0-3): 4 x ProductLayer(\n",
      "        (linear): MLP(\n",
      "          (fcs): ModuleList(\n",
      "            (0): Conv2d(34, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mixer_skips): ModuleList(\n",
      "      (0-3): 4 x SoftGating()\n",
      "    )\n",
      "  )\n",
      "  (lifting): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (projection): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.StepLR object at 0x7fd8cb1bd660>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.training.losses.H1Loss object at 0x7fd8cb1bc430>\n",
      "\n",
      " * Test: {'h1': <neuralop.training.losses.H1Loss object at 0x7fd8cb1bc430>, 'l2': <neuralop.training.losses.LpLoss object at 0x7fd8cb1bcf70>}\n"
     ]
    }
   ],
   "source": [
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=1e-3, \n",
    "                                weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "localtime = time.localtime(time.time())\n",
    "time_now = f\"{localtime.tm_mon}-{localtime.tm_mday}-{localtime.tm_hour}-{localtime.tm_min}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 15:26:07.688335: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-19 15:26:07.815762: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-19 15:26:07.815802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-19 15:26:07.836270: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-19 15:26:07.881064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 15:26:08.418147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard method to load data to device.\n",
      "using standard method to compute loss.\n",
      "self.override_load_to_device=False\n",
      "self.overrides_loss=False\n"
     ]
    }
   ],
   "source": [
    "from neuralop.training import MultipleInputCallback, SimpleTensorBoardLoggerCallback\n",
    "trainer = Trainer(model=model, n_epochs=500,\n",
    "                  device=device,\n",
    "                  callbacks=[MultipleInputCallback(append_positional_encoding=True), SimpleTensorBoardLoggerCallback(log_dir='runs/TorisLi_exp_'+model_name+time_now,)],             \n",
    "                  wandb_log=False,\n",
    "                  log_test_interval=4,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 19000 samples\n",
      "Testing on [3800] samples         on resolutions [64].\n",
      "Raw outputs of size out.shape=torch.Size([32, 1, 64, 64])\n",
      "[0] time=7.22, avg_loss=13.8714, train_err=11.6763, 64_h1=0.2767, 64_l2=0.1016\n",
      "[4] time=6.28, avg_loss=7.4647, train_err=6.2834, 64_h1=0.1992, 64_l2=0.0669\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43meval_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/cfd/myNeuralOperator/neuralop/training/trainer.py:170\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, test_loaders, optimizer, scheduler, regularizer, training_loss, eval_losses)\u001b[0m\n\u001b[1;32m    168\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msample)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_before_loss(out\u001b[38;5;241m=\u001b[39mout)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/cfd/myNeuralOperator/neuralop/models/new_fno.py:258\u001b[0m, in \u001b[0;36mFNO.forward\u001b[0;34m(self, x, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding\u001b[38;5;241m.\u001b[39mpad(x)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfno_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding\u001b[38;5;241m.\u001b[39munpad(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/cfd/myNeuralOperator/neuralop/layers/fno_block.py:518\u001b[0m, in \u001b[0;36mFNOBlocks1.forward\u001b[0;34m(self, x, index, output_shape)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_with_preactivation(x, index, output_shape)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_postactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/cfd/myNeuralOperator/neuralop/layers/fno_block.py:521\u001b[0m, in \u001b[0;36mFNOBlocks1.forward_with_postactivation\u001b[0;34m(self, x, index, output_shape)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_with_postactivation\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, output_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 521\u001b[0m     x_skip_fno \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfno_skips\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     x_skip_fno \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[index]\u001b[38;5;241m.\u001b[39mtransform(x_skip_fno, output_shape\u001b[38;5;241m=\u001b[39moutput_shape)\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_mixer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;66;03m# print(self.mixer_skips)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders={64: test_loader},\n",
    "              optimizer=optimizer, \n",
    "              scheduler=scheduler, \n",
    "              regularizer=False, \n",
    "              training_loss=train_loss, \n",
    "              eval_losses=eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./test_param_search/b_64/mode_21/prod_2/layer_4/hid_32/lift_256/proj_64/fact-/rank_0.42/mix-/pos-enc-True/lr_0.001/wd_0/sche-step_100/gamma_0.5/loss-h1/TorusLi_FNO_1-17-19-20.pth'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "temp_file_path = os.path.join(temp_dir, 'temp_model.pth')\n",
    "torch.save(trainer.model.state_dict(), temp_file_path)\n",
    "\n",
    "target_path = './test_param_search/b_64/mode_21/prod_2/layer_4/hid_32/lift_256/proj_64/fact-/rank_0.42/mix-/pos-enc-True/lr_0.001/wd_0/sche-step_100/gamma_0.5/loss-h1/TorusLi_FNO_1-17-19-20.pth'\n",
    "\n",
    "target_dir = os.path.dirname(target_path)\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "shutil.move(temp_file_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = './test_param_search/b_64/mode_21/prod_2/TorusLi_FNO_1-17-19-20.pth'\n",
    "\n",
    "# Create directories if they do not exist\n",
    "dir_path = os.path.dirname(path)\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(trainer.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.model.state_dict(), \"./ckpt/TorisLi/\"+model_name+\"_ep500.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_44388/849268122.py\", line 11, in <module>\n",
      "    out = model(x.unsqueeze(0))\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yichen/repo/cfd/myNeuralOperator/neuralop/models/new_fno.py\", line 252, in forward\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yichen/repo/cfd/myNeuralOperator/neuralop/layers/mlp.py\", line 62, in forward\n",
      "    x = fc(x)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Given groups=1, weight of size [256, 3, 1, 1], expected input[1, 1, 64, 64] to have 3 channels, but got 1 channels instead\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/yichen/anaconda3/envs/test/lib/python3.10/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = test_loader.dataset\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "for index in range(3):\n",
    "    data = test_samples[index]\n",
    "    # Input x\n",
    "    x = data['x']\n",
    "    # Ground-truth\n",
    "    y = data['y']\n",
    "    # Model prediction\n",
    "    out = model(x.unsqueeze(0))\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, index*3 + 1)\n",
    "    ax.imshow(x[0], cmap='gray')\n",
    "    if index == 0: \n",
    "        ax.set_title('Input x')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, index*3 + 2)\n",
    "    ax.imshow(y.squeeze())\n",
    "    if index == 0: \n",
    "        ax.set_title('Ground-truth y')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, index*3 + 3)\n",
    "    ax.imshow(out.squeeze().detach().numpy())\n",
    "    if index == 0: \n",
    "        ax.set_title('Model prediction')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "fig.suptitle('Inputs, ground-truth output and prediction.', y=0.98)\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
